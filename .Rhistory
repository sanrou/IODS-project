# *Affect    Dg + Dh
# *Affect toward statistics
learning2014$affect <- lrn14$Dg + lrn14$Dh
learning2014$lar <- data.frame(lrn14$Aa + lrn14$Ab + lrn14$Ad)
lar <- lrn14$Aa + lrn14$Ab + lrn14$Ad
learning2014$lar <- lar
learning2014$lar <- data.frame(lar)
lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
learning2014 <- data.frame(lar)
learning2014$lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
learning2014$lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
#   *d_sm      D03 + D11 + D19 + D27
# *Seeking Meaning
learning2014$d_sm <- lrn14$D03 + lrn14$D11 + lrn14$D19 + lrn14$D27
# *d_ri      D07 + D14 + D22 + D30
# *Relating Ideas
learning2014$d_ri <- lrn14$D07 + lrn14$D14 + lrn14$D22 + lrn14$D30
# *d_ue      D06 + D15 + D23 + D31
# *Use of Evidence
learning2014$d_ue <- lrn14$D06 + lrn14$D15 + lrn14$D23 + lrn14$D31
# *su_lp     SU02 + SU10 + SU18 + SU26
# *Lack of Purpose
learning2014$su_lp <- lrn14$SU02 + lrn14$SU10 + lrn14$SU18 + lrn14$SU26
# *su_um     SU05 + SU13 + SU21 + SU29
# *Unrelated Memorising
learning2014$su_um <- lrn14$SU05 + lrn14$SU13 + lrn14$SU21 + lrn14$SU29
# *su_sb     SU08 + SU16 + SU24 + SU32
# *Syllabus-boundness
learning2014$su_sb <- lrn14$SU08 + lrn14$SU16 + lrn14$SU24 + lrn14$SU32
# *st_os     ST01 + ST09 + ST17 + ST25
# *Organized Studying
learning2014$st_os <- lrn14$ST01 + lrn14$ST09 + lrn14$ST17 + lrn14$ST25
# *st_tm     ST04 + ST12 + ST20 + ST28
# *Time Management
learning2014$st_tm <- lrn14$ST04 + lrn14$ST12 + lrn14$ST20 + lrn14$ST28
# *Deep      d_sm + d_ri + d_ue (min = 12, max = 60)
# *Deep approach
learning2014$deep <- lrn14$d_sm + lrn14$d_ri + lrn14$d_ue
# *Surface   su_lp + su_um + su_sb (min =12, max = 60)
# *Surface approach
learning2014$surface <- lrn14$su_lp + lrn14$su_um + lrn14$su_sb
# *Strategic           st_os + st_tm (min = 8, max = 40)
# *Strategic approach
learning2014$strategic <- lrn14$st_os + lrn14$st_tm
#   *Su_Ti     Ca + Cd + Ce + Ch
# *Supporting understanding (related to a deep approach)
learning2014$su_ti <- lrn14$Ca + lrn14$Cd + lrn14$Ce + lrn14$Ch
# *D_Su      Cb + Cc + Cf + Cg
# *Transmitting information (related to a surface approach)
learning2014$d_su <- lrn14$Cb + lrn14$Cc + lrn14$Cf + lrn14$Cg
#   *Obs!! Df and Dh must be reversed first:
lrn14$Df <- 6 - lrn14$Df
lrn14$Dh <- 6 - lrn14$Dh
#   *Stat_Conf           Da + Df
# *Confidence in doing statistics
learning2014$stat_conf <- lrn14$Da + lrn14$Df
# *Value     Db + Dj
# *Value of statistics
learning2014$value <- lrn14$Db + lrn14$Dj
# *Interest  Dc + De
# *Interest in statistics
learning2014$interest <- lrn14$Dc + lrn14$De
# *Math_Conf           Dd + Di
# *Confidence in doing math
learning2014$math_conf <- lrn14$Dd + lrn14$Di
# *Affect    Dg + Dh
# *Affect toward statistics
learning2014$affect <- lrn14$Dg + lrn14$Dh
learning2014 <- scale(learning2014)
View(learning2014)
take_columns <- c("gender","Age","Points")
learning2014 <- select(lrn14, one_of(take_columns))
lrn14(take_columns)
lrn14[take_columns]
learning2014 <- lrn14[take_columns]
learning2014$interest <- lrn14$Dc + lrn14$De
learning2014 <- lrn14[take_columns]
learning2014$affect <- lrn14$Dg + lrn14$Dh
learning2014 <- merge(learning2014, lrn14[take_columns])
View(learning2014)
?merge
lar <- lrn14$Aa + lrn14$Ab + lrn14$Ad
learning2014 <- data.frame(lar)
learning2014 <- cbind(learning2014, lrn14[take_columns])
View(learning2014)
colnames(learning2014)=='Age' <- 'age'
TRUE == FALSE
colnames(learning2014)['Age'] <- 'age'
names(learning2014)[names(learning2014) == 'Age'] <- 'age'
View(learning2014)
names(learning2014)[names(learning2014) == 'Points'] <- 'points'
# Santeri Rouhinen, 2017.12.12. santeri.rouhinen@helsinki.fi
# Load learning dataset and wrangle it.
# Link with information about the dataset:
# http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS2-meta.txt
# Access libraries
library(dplyr)
# read the data into memory
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# lrn14 has 183 observations with 60 variables (questions of learning strategy and self evaluations)
### Collapse questions into guestion classes. Keep gender, age, attitude and points.
## Collapse some of the variables like in the JYTOPKYS2-meta.txt file:
#   *Lar       Aa + Ac + Ad
# *Learning as Reproducing
lar <- lrn14$Aa + lrn14$Ab + lrn14$Ad
learning2014 <- data.frame(lar)
# *Lat       Ab + Ae + Af
# *Learning as Transforming
learning2014$lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
#   *d_sm      D03 + D11 + D19 + D27
# *Seeking Meaning
learning2014$d_sm <- lrn14$D03 + lrn14$D11 + lrn14$D19 + lrn14$D27
# *d_ri      D07 + D14 + D22 + D30
# *Relating Ideas
learning2014$d_ri <- lrn14$D07 + lrn14$D14 + lrn14$D22 + lrn14$D30
# *d_ue      D06 + D15 + D23 + D31
# *Use of Evidence
learning2014$d_ue <- lrn14$D06 + lrn14$D15 + lrn14$D23 + lrn14$D31
# *su_lp     SU02 + SU10 + SU18 + SU26
# *Lack of Purpose
learning2014$su_lp <- lrn14$SU02 + lrn14$SU10 + lrn14$SU18 + lrn14$SU26
# *su_um     SU05 + SU13 + SU21 + SU29
# *Unrelated Memorising
learning2014$su_um <- lrn14$SU05 + lrn14$SU13 + lrn14$SU21 + lrn14$SU29
# *su_sb     SU08 + SU16 + SU24 + SU32
# *Syllabus-boundness
learning2014$su_sb <- lrn14$SU08 + lrn14$SU16 + lrn14$SU24 + lrn14$SU32
# *st_os     ST01 + ST09 + ST17 + ST25
# *Organized Studying
learning2014$st_os <- lrn14$ST01 + lrn14$ST09 + lrn14$ST17 + lrn14$ST25
# *st_tm     ST04 + ST12 + ST20 + ST28
# *Time Management
learning2014$st_tm <- lrn14$ST04 + lrn14$ST12 + lrn14$ST20 + lrn14$ST28
# *Deep      d_sm + d_ri + d_ue (min = 12, max = 60)
# *Deep approach
learning2014$deep <- lrn14$d_sm + lrn14$d_ri + lrn14$d_ue
# *Surface   su_lp + su_um + su_sb (min =12, max = 60)
# *Surface approach
learning2014$surface <- lrn14$su_lp + lrn14$su_um + lrn14$su_sb
# *Strategic           st_os + st_tm (min = 8, max = 40)
# *Strategic approach
learning2014$strategic <- lrn14$st_os + lrn14$st_tm
#   *Su_Ti     Ca + Cd + Ce + Ch
# *Supporting understanding (related to a deep approach)
learning2014$su_ti <- lrn14$Ca + lrn14$Cd + lrn14$Ce + lrn14$Ch
# *D_Su      Cb + Cc + Cf + Cg
# *Transmitting information (related to a surface approach)
learning2014$d_su <- lrn14$Cb + lrn14$Cc + lrn14$Cf + lrn14$Cg
#   *Obs!! Df and Dh must be reversed first:
lrn14$Df <- 6 - lrn14$Df
lrn14$Dh <- 6 - lrn14$Dh
#   *Stat_Conf           Da + Df
# *Confidence in doing statistics
learning2014$stat_conf <- lrn14$Da + lrn14$Df
# *Value     Db + Dj
# *Value of statistics
learning2014$value <- lrn14$Db + lrn14$Dj
# *Interest  Dc + De
# *Interest in statistics
learning2014$interest <- lrn14$Dc + lrn14$De
# *Math_Conf           Dd + Di
# *Confidence in doing math
learning2014$math_conf <- lrn14$Dd + lrn14$Di
# *Affect    Dg + Dh
# *Affect toward statistics
learning2014$affect <- lrn14$Dg + lrn14$Dh
##! Scale the variables to a mean of 0 and SD of 1.
learning2014 <- scale(learning2014)
## Take original values from lrn14
take_columns <- c("gender","Age","Points")
learning2014 <- cbind(learning2014, lrn14[take_columns])
# Rename headers so that Age and Points are not capitalized.
names(learning2014)[names(learning2014) == 'Age'] <- 'age'
names(learning2014)[names(learning2014) == 'Points'] <- 'points'
names(learning2014)
blaa <- names(learning2014)
sapply(blaa, toupper)
blaa
names(learning2014) %>% sapply(tolower)
# Santeri Rouhinen, 2017.12.12. santeri.rouhinen@helsinki.fi
# Load learning dataset and wrangle it.
# Link with information about the dataset:
# http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS2-meta.txt
# Access libraries
library(dplyr)
# read the data into memory
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# lrn14 has 183 observations with 60 variables (questions of learning strategy and self evaluations)
### Collapse questions into guestion classes.
## Collapse most of the variables like in the JYTOPKYS2-meta.txt file:
#   *Lar       Aa + Ac + Ad    *Learning as Reproducing
lar <- lrn14$Aa + lrn14$Ab + lrn14$Ad
learning2014 <- data.frame(lar)
# *Lat       Ab + Ae + Af      *Learning as Transforming
learning2014$lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
#   *d_sm      D03 + D11 + D19 + D27     *Seeking Meaning
learning2014$d_sm <- lrn14$D03 + lrn14$D11 + lrn14$D19 + lrn14$D27
# *d_ri      D07 + D14 + D22 + D30       *Relating Ideas
learning2014$d_ri <- lrn14$D07 + lrn14$D14 + lrn14$D22 + lrn14$D30
# *d_ue      D06 + D15 + D23 + D31       *Use of Evidence
learning2014$d_ue <- lrn14$D06 + lrn14$D15 + lrn14$D23 + lrn14$D31
# *su_lp     SU02 + SU10 + SU18 + SU26   *Lack of Purpose
learning2014$su_lp <- lrn14$SU02 + lrn14$SU10 + lrn14$SU18 + lrn14$SU26
# *su_um     SU05 + SU13 + SU21 + SU29   *Unrelated Memorising
learning2014$su_um <- lrn14$SU05 + lrn14$SU13 + lrn14$SU21 + lrn14$SU29
# *su_sb     SU08 + SU16 + SU24 + SU32   *Syllabus-boundness
learning2014$su_sb <- lrn14$SU08 + lrn14$SU16 + lrn14$SU24 + lrn14$SU32
# *st_os     ST01 + ST09 + ST17 + ST25   *Organized Studying
learning2014$st_os <- lrn14$ST01 + lrn14$ST09 + lrn14$ST17 + lrn14$ST25
# *st_tm     ST04 + ST12 + ST20 + ST28   *Time Management
learning2014$st_tm <- lrn14$ST04 + lrn14$ST12 + lrn14$ST20 + lrn14$ST28
# *Deep      d_sm + d_ri + d_ue (min = 12, max = 60)     *Deep approach
learning2014$deep <- lrn14$d_sm + lrn14$d_ri + lrn14$d_ue
# *Surface   su_lp + su_um + su_sb (min =12, max = 60)   *Surface approach
learning2014$surface <- lrn14$su_lp + lrn14$su_um + lrn14$su_sb
# *Strategic           st_os + st_tm (min = 8, max = 40) *Strategic approach
learning2014$strategic <- lrn14$st_os + lrn14$st_tm
#   *Su_Ti     Ca + Cd + Ce + Ch   *Supporting understanding (related to a deep approach)
learning2014$su_ti <- lrn14$Ca + lrn14$Cd + lrn14$Ce + lrn14$Ch
# *D_Su      Cb + Cc + Cf + Cg     *Transmitting information (related to a surface approach)
learning2014$d_su <- lrn14$Cb + lrn14$Cc + lrn14$Cf + lrn14$Cg
#   *Obs!! Df and Dh must be reversed first:
lrn14$Df <- 6 - lrn14$Df
lrn14$Dh <- 6 - lrn14$Dh
#   *Stat_Conf           Da + Df  *Confidence in doing statistics
learning2014$stat_conf <- lrn14$Da + lrn14$Df
# *Value     Db + Dj              *Value of statistics
learning2014$value <- lrn14$Db + lrn14$Dj
# *Interest  Dc + De             *Interest in statistics
learning2014$interest <- lrn14$Dc + lrn14$De
# *Math_Conf           Dd + Di   *Confidence in doing math
learning2014$math_conf <- lrn14$Dd + lrn14$Di
# *Affect    Dg + Dh             *Affect toward statistics
learning2014$affect <- lrn14$Dg + lrn14$Dh
### Scale the variables to a mean of 0 and SD of 1.
learning2014 <- scale(learning2014)
### Take original values from lrn14
take_columns <- c("gender","Age","Points")
learning2014 <- cbind(learning2014, lrn14[take_columns])
# Rename headers so that Age and Points are not capitalized.
names(learning2014) %>% sapply(tolower)
View(learning2014)
names(learning2014) <- sapply(names(learning2014, tolower))
(names(learning2014, tolower)
(names(learning2014, tolower))
names(learning2014, tolower))
names(learning2014, tolower))
sapply(names(learning2014, tolower))
names(learning2014) <- sapply(names(learning2014), tolower)
View(learning2014)
rm(list = ls())
# Santeri Rouhinen, 2017.12.12. santeri.rouhinen@helsinki.fi
# Load learning dataset and wrangle it.
# Link with information about the dataset:
# http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS2-meta.txt
# Access libraries
library(dplyr)
# read the data into memory
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# lrn14 has 183 observations with 60 variables (questions of learning strategy and self evaluations)
### Collapse questions into guestion classes.
## Collapse most of the variables like in the JYTOPKYS2-meta.txt file:
#   *Lar       Aa + Ac + Ad    *Learning as Reproducing
lar <- lrn14$Aa + lrn14$Ab + lrn14$Ad
learning2014 <- data.frame(lar)
# *Lat       Ab + Ae + Af      *Learning as Transforming
learning2014$lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
#   *d_sm      D03 + D11 + D19 + D27     *Seeking Meaning
learning2014$d_sm <- lrn14$D03 + lrn14$D11 + lrn14$D19 + lrn14$D27
# *d_ri      D07 + D14 + D22 + D30       *Relating Ideas
learning2014$d_ri <- lrn14$D07 + lrn14$D14 + lrn14$D22 + lrn14$D30
# *d_ue      D06 + D15 + D23 + D31       *Use of Evidence
learning2014$d_ue <- lrn14$D06 + lrn14$D15 + lrn14$D23 + lrn14$D31
# *su_lp     SU02 + SU10 + SU18 + SU26   *Lack of Purpose
learning2014$su_lp <- lrn14$SU02 + lrn14$SU10 + lrn14$SU18 + lrn14$SU26
# *su_um     SU05 + SU13 + SU21 + SU29   *Unrelated Memorising
learning2014$su_um <- lrn14$SU05 + lrn14$SU13 + lrn14$SU21 + lrn14$SU29
# *su_sb     SU08 + SU16 + SU24 + SU32   *Syllabus-boundness
learning2014$su_sb <- lrn14$SU08 + lrn14$SU16 + lrn14$SU24 + lrn14$SU32
# *st_os     ST01 + ST09 + ST17 + ST25   *Organized Studying
learning2014$st_os <- lrn14$ST01 + lrn14$ST09 + lrn14$ST17 + lrn14$ST25
# *st_tm     ST04 + ST12 + ST20 + ST28   *Time Management
learning2014$st_tm <- lrn14$ST04 + lrn14$ST12 + lrn14$ST20 + lrn14$ST28
# *Deep      d_sm + d_ri + d_ue (min = 12, max = 60)     *Deep approach
learning2014$deep <- lrn14$d_sm + lrn14$d_ri + lrn14$d_ue
# *Surface   su_lp + su_um + su_sb (min =12, max = 60)   *Surface approach
learning2014$surface <- lrn14$su_lp + lrn14$su_um + lrn14$su_sb
# *Strategic           st_os + st_tm (min = 8, max = 40) *Strategic approach
learning2014$strategic <- lrn14$st_os + lrn14$st_tm
#   *Su_Ti     Ca + Cd + Ce + Ch   *Supporting understanding (related to a deep approach)
learning2014$su_ti <- lrn14$Ca + lrn14$Cd + lrn14$Ce + lrn14$Ch
# *D_Su      Cb + Cc + Cf + Cg     *Transmitting information (related to a surface approach)
learning2014$d_su <- lrn14$Cb + lrn14$Cc + lrn14$Cf + lrn14$Cg
#   *Obs!! Df and Dh must be reversed first:
lrn14$Df <- 6 - lrn14$Df
lrn14$Dh <- 6 - lrn14$Dh
#   *Stat_Conf           Da + Df  *Confidence in doing statistics
learning2014$stat_conf <- lrn14$Da + lrn14$Df
# *Value     Db + Dj              *Value of statistics
learning2014$value <- lrn14$Db + lrn14$Dj
# *Interest  Dc + De             *Interest in statistics
learning2014$interest <- lrn14$Dc + lrn14$De
# *Math_Conf           Dd + Di   *Confidence in doing math
learning2014$math_conf <- lrn14$Dd + lrn14$Di
# *Affect    Dg + Dh             *Affect toward statistics
learning2014$affect <- lrn14$Dg + lrn14$Dh
## Scale the variables to a mean of 0 and SD of 1.
learning2014 <- scale(learning2014)
## Take original values from lrn14
take_columns <- c("gender","Age","Points")
learning2014 <- cbind(learning2014, lrn14[take_columns])
## Rename headers so that Age and Points are not capitalized.
names(learning2014) <- sapply(names(learning2014), tolower)
## Save data. There should be only a few files so save to root
write.csv(learning2014, file = "learning2014.csv")
View(learning2014)
rm(list = ls())
read.csv("learning2014.csv")
learning2014 <- read.csv("learning2014.csv")
View(learning2014)
learning2014 <- read.csv("learning2014.csv",row.names = 1)
View(learning2014)
count(learning2014$â˜ºpoints == 0)
count(learning2014$points == 0)
count(learning2014$points == 2)
learning2014$points == 0
?count
sum(learning2014$points == 0)
examY <- learning2014$points == 0
examY <- learning2014[learning2014$points == 0]
library(dplyr)
examY <- filter(learning2014, points > 0)
examN <- filter(learning2014, points == 0)
yTrainN <- round(nrow(examY)*0.75)
examY[yTrainN:end]
examY[yTrainN:-1]
trainSet <- cbind(examY[1:yTrainN], examN[1:nTrainN])
1:yTrainN
1:nTrainN
nTrainN <- round(nrow(examN)*0.75)
nTestN  <- nrow(examN) - nTrainN
1:nTrainN
trainSet <- cbind(examY[1:yTrainN], examN[1:nTrainN])
examY[1:yTrainN]
examY <- filter(learning2014, points > 0)
examN <- filter(learning2014, points == 0)
examN[1]
trainSet <- cbind(examY[,1:yTrainN], examN[,1:nTrainN])
trainSet <- cbind(examY[, 1:yTrainN], examN[, 1:nTrainN])
examY[, 1:yTrainN]
yTrainN <- round(nrow(examY)*0.75)
examN[,1]
examN[,1:2]
examN[1:2,]
trainSet <- cbind(examY[1:yTrainN ,], examN[1:nTrainN ,])
examY[1:yTrainN ,]
examN[1:nTrainN ,]
cbind?
?cbind
trainSet <- rbind(examY[1:yTrainN ,], examN[1:nTrainN ,])
testSet  <- rbind(examY[-(1:yTrainN) ,], examN[-(1:nTrainN) ,])
View(learning2014)
46*4
46*3
View(learning2014)
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
View(lrn14)
# Santeri Rouhinen, 2017.12.12. santeri.rouhinen@helsinki.fi
# Load learning dataset and wrangle it.
# Link with information about the dataset:
# http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS2-meta.txt
# Access libraries
library(dplyr)
# read the data into memory
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# lrn14 has 183 observations with 60 variables (questions of learning strategy and self evaluations)
### Collapse questions into guestion classes.
## Collapse most of the variables like in the JYTOPKYS2-meta.txt file:
#   *Lar       Aa + Ac + Ad    *Learning as Reproducing
lar <- lrn14$Aa + lrn14$Ab + lrn14$Ad
learning2014 <- data.frame(lar)
# *Lat       Ab + Ae + Af      *Learning as Transforming
learning2014$lat <- lrn14$Ab + lrn14$Ae + lrn14$Af
#   *d_sm      D03 + D11 + D19 + D27     *Seeking Meaning
learning2014$d_sm <- lrn14$D03 + lrn14$D11 + lrn14$D19 + lrn14$D27
# *d_ri      D07 + D14 + D22 + D30       *Relating Ideas
learning2014$d_ri <- lrn14$D07 + lrn14$D14 + lrn14$D22 + lrn14$D30
# *d_ue      D06 + D15 + D23 + D31       *Use of Evidence
learning2014$d_ue <- lrn14$D06 + lrn14$D15 + lrn14$D23 + lrn14$D31
# *su_lp     SU02 + SU10 + SU18 + SU26   *Lack of Purpose
learning2014$su_lp <- lrn14$SU02 + lrn14$SU10 + lrn14$SU18 + lrn14$SU26
# *su_um     SU05 + SU13 + SU21 + SU29   *Unrelated Memorising
learning2014$su_um <- lrn14$SU05 + lrn14$SU13 + lrn14$SU21 + lrn14$SU29
# *su_sb     SU08 + SU16 + SU24 + SU32   *Syllabus-boundness
learning2014$su_sb <- lrn14$SU08 + lrn14$SU16 + lrn14$SU24 + lrn14$SU32
# *st_os     ST01 + ST09 + ST17 + ST25   *Organized Studying
learning2014$st_os <- lrn14$ST01 + lrn14$ST09 + lrn14$ST17 + lrn14$ST25
# *st_tm     ST04 + ST12 + ST20 + ST28   *Time Management
learning2014$st_tm <- lrn14$ST04 + lrn14$ST12 + lrn14$ST20 + lrn14$ST28
#   *Su_Ti     Ca + Cd + Ce + Ch   *Supporting understanding (related to a deep approach)
learning2014$su_ti <- lrn14$Ca + lrn14$Cd + lrn14$Ce + lrn14$Ch
# *D_Su      Cb + Cc + Cf + Cg     *Transmitting information (related to a surface approach)
learning2014$d_su <- lrn14$Cb + lrn14$Cc + lrn14$Cf + lrn14$Cg
#   *Obs!! Df and Dh must be reversed first:
lrn14$Df <- 6 - lrn14$Df
lrn14$Dh <- 6 - lrn14$Dh
#   *Stat_Conf           Da + Df  *Confidence in doing statistics
learning2014$stat_conf <- lrn14$Da + lrn14$Df
# *Value     Db + Dj              *Value of statistics
learning2014$value <- lrn14$Db + lrn14$Dj
# *Interest  Dc + De             *Interest in statistics
learning2014$interest <- lrn14$Dc + lrn14$De
# *Math_Conf           Dd + Di   *Confidence in doing math
learning2014$math_conf <- lrn14$Dd + lrn14$Di
# *Affect    Dg + Dh             *Affect toward statistics
learning2014$affect <- lrn14$Dg + lrn14$Dh
## Scale the variables to a mean of 0 and SD of 1.
learning2014 <- scale(learning2014)
## Take original values from lrn14
take_columns <- c("gender","Age","Points")
learning2014 <- cbind(learning2014, lrn14[take_columns])
## Rename column names so that Age and Points are not capitalized.
names(learning2014) <- sapply(names(learning2014), tolower)
## Save data. There should be only a few files so save to root.
# The data now has 183 observations (students) and 20 variables.
write.csv(learning2014, file = "learning2014.csv")
library(dplyr)
# Read learning2014.csv
learning2014 <- read.csv("learning2014.csv",row.names = 1)
# Check how many did not take the exam. Answer is 17.
sum(learning2014$points == 0)
# Split the learning2014 to exam takers and non-takers.
examY <- filter(learning2014, points > 0)
examN <- filter(learning2014, points == 0)
# Lets hope that the observations are not sorted in some way. Use the first 75 % as the training set and the latter 25 % as the testing set.
yTrainN <- round(nrow(examY)*0.75)
nTrainN <- round(nrow(examN)*0.75)
trainSet <- rbind(examY[1:yTrainN ,], examN[1:nTrainN ,])
testSet  <- rbind(examY[-(1:yTrainN) ,], examN[-(1:nTrainN) ,])
# Select variables for the hand picked variables for the hopefully better performing model.
colnames(learning2014)
trainSet$takeExam <- trainSet$points > 0
trainSet$points <- []
trainSet <- dplyr::select(trainSet, -points)
fullModel <- glm(takeExam ~ ., data = trainSet, family = "binomial")
handModel <- glm(takeExam ~ subVariables, data = trainSet, family = "binomial")
subVariables <- c("su_lp", "st_os", "st_tm", "stat_conf", "value", "interest", "math_conf", "affect")
handModel <- glm(takeExam ~ subVariables, data = trainSet, family = "binomial")
probabilities <- predict(fullModel, type = "response")
handSet  <- trainSet[subVariables]
handSet  <- trainSet[subVariables + handSet]
handSet  <- trainSet[subVariables, handSet]
handSet  <- trainSet[c(subVariables, handSet)]
handSet  <- trainSet[c(subVariables, 'takeExam')]
View(handSet)
# Temp for the final assignment.
# Initialize required libraries
library(dplyr)
# Read learning2014.csv
learning2014 <- read.csv("learning2014.csv",row.names = 1)
# Check how many did not take the exam. Answer is 17.
sum(learning2014$points == 0)
# Split the learning2014 to exam takers and non-takers.
examY <- filter(learning2014, points > 0)
examN <- filter(learning2014, points == 0)
# Lets hope that the observations are not sorted in some way. Use the first 75 % as the training set and the latter 25 % as the testing set.
yTrainN <- round(nrow(examY)*0.75)
nTrainN <- round(nrow(examN)*0.75)
trainSet <- rbind(examY[1:yTrainN ,], examN[1:nTrainN ,])
testSet  <- rbind(examY[-(1:yTrainN) ,], examN[-(1:nTrainN) ,])
## Select variables for the hand picked variables for the hopefully better performing model.
# First list the columns
colnames(learning2014)
# Hand pick promising variables
subVariables <- c("su_lp", "st_os", "st_tm", "stat_conf", "value", "interest", "math_conf", "affect")
# Change points to be boolean of taking exam
trainSet$takeExam <- trainSet$points > 0
# Remove the points variable from trainSet, so that the full model is not able to use it as a variable.
trainSet <- dplyr::select(trainSet, -points)
# Split a subset of variables for the hand picked model.
handSet  <- trainSet[c(subVariables, 'takeExam')]
## I'll use a linear model. Logistic regression model (glm).
fullModel <- glm(takeExam ~ ., data = trainSet, family = "binomial")
handModel <- glm(takeExam ~ ., data = handSet,  family = "binomial")
probabilitiesHand <- predict(handModel, type = "response")
probabilitiesFull <- predict(fullModel, type = "response")
probabilitiesFull <- predict(fullModel, type = "response")
probabilitiesHand <- predict(handModel, type = "response")
trainSet <- mutate(trainSet, probability = probabilitiesFull)
handSet  <- mutate(handSet,  probability = probabilitiesHand)
trainSet <- mutate(trainSet, prediction = probability > 0.5)
handSet  <- mutate(handSet,  prediction = probability > 0.5)
table(takeExam = trainSet$takeExam, prediction = trainSet$prediction)
table(takeExam = handSet$takeExam,  prediction = handSet$prediction)
fullProbabilitySorted <- sort(trainSet$probability)
thresholdFull <- mean(fullProbabilitySorted[13,14])
thresholdFull <- mean(fullProbabilitySorted[c(13,14)])
head(fullProbabilitySorted)
head(fullProbabilitySorted,20)
handProbabilitySorted <- sort(handSet$probability)
thresholdHand <- mean(handProbabilitySorted[c(13,14)])
trainSet <- mutate(trainSet, prediction = probability > thresholdFull)
handSet  <- mutate(handSet,  prediction = probability > thresholdHand)
table(takeExam = trainSet$takeExam, prediction = trainSet$prediction)
table(takeExam = handSet$takeExam,  prediction = handSet$prediction)
predFull <- predict(fullModel, newdata = testSet)
predHand <- predict(handModel, newdata = testSet)
predFull
table(correct = correct_classes, predicted = lda.pred$class)
View(testSet)
View(trainSet)
View(testSet)
